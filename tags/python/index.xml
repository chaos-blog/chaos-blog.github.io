<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Chaos</title>
    <link>/tags/python/</link>
    <description>Recent content in Python on Chaos</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language><atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>model.zero_grad() VS. optimzer.zero_grad()</title>
      <link>/posts/12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/12/</guid>
      <description>引言 在模型训练时，每个Batch反向传播完成后我们需要手动清除计算图上本次迭代的所有梯度 在阅读不同的代码时，总能看到不同的清空代码： model.zero_grad() optimizer.zero_grad() 正文 上述两种梯度清空的方式均有效，区别在于起作用的范围不同 model.zero_grad() 此时mdoel包含的所有参数上的梯度均被清空 optimizer.zero_grad() 此时该优化器中负责更新的模型参数上的梯度被清空，即不一定是全部的梯度被清空 若模型训练过程中只有一个优化器，即优化器构造时使用 optimizer = optim.Optimiers(model.parameters(), lr=args.lr) 此时上述两种梯度清空方式完全等价 总结 两种不同的梯度清零方式在多数场景下基本等价，其区别在于作用范围不同 对于多任务训练、多优化器的训练中，需要根据具体训练策略的不同对参数梯度进行不同作用范围的清空 </description>
      <content>&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在模型训练时，每个Batch反向传播完成后我们需要手动清除计算图上本次迭代的所有梯度&lt;/li&gt;
&lt;li&gt;在阅读不同的代码时，总能看到不同的清空代码：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;上述两种梯度清空的方式均有效，区别在于起作用的范围不同&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model.zero_grad()&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;此时&lt;code&gt;mdoel&lt;/code&gt;包含的所有参数上的梯度均被清空&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;optimizer.zero_grad()&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;此时该优化器中负责更新的模型参数上的梯度被清空，即不一定是全部的梯度被清空&lt;/li&gt;
&lt;li&gt;若模型训练过程中只有一个优化器，即优化器构造时使用&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;   optimizer = optim.Optimiers(model.parameters(), lr=args.lr)
&lt;/code&gt;&lt;/pre&gt;此时上述两种梯度清空方式完全等价&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;两种不同的梯度清零方式在多数场景下基本等价，其区别在于作用范围不同&lt;/li&gt;
&lt;li&gt;对于多任务训练、多优化器的训练中，需要根据具体训练策略的不同对参数梯度进行不同作用范围的清空&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>python中的print()函数深度使用</title>
      <link>/posts/6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/6/</guid>
      <description>背景 今天读代码的过程中发现了一种很有意思的print()写法 每一个程序员对每一种语言最熟悉的不过于各种“hello world”，这也不可避免的造成了我们对最熟悉的print()语句最为“陌生” 主题 对于python中print()函数的参数深度使用方法 语法 print(*values, [sep=&amp;quot;&amp;quot;, end=&amp;quot;&amp;quot;, file=&amp;quot;&amp;quot;, flush=&amp;quot;&amp;quot;]) 参数 *values 必需 要输出的内容，可以是任何类型对象 同时输出多个对象时，需要用,隔开 sep 可选 同时输出多个对象时的分隔符 默认空格 end 可选 输出最后一个对象之后的结尾符 默认是一个\n，即换行 file 可选 要打印输出到的“设备” 默认为输出到当前终端，可以指定已打开的文件对象 对文件是否追加更新由上下文的open()中的a/w参数指定 flush 可选 整成情况下输出是否被缓存（是否等待文件对象关闭前同一写入）由file对象决定，但如果指定为Ture则print()会强制立即将内容刷新进文件 一般用不到 </description>
      <content>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;今天读代码的过程中发现了一种很有意思的print()写法&lt;/li&gt;
&lt;li&gt;每一个程序员对每一种语言最熟悉的不过于各种“hello world”，这也不可避免的造成了我们对最熟悉的print()语句最为“陌生”&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;主题&#34;&gt;主题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对于python中print()函数的参数深度使用方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;print(*values, [sep=&amp;quot;&amp;quot;, end=&amp;quot;&amp;quot;, file=&amp;quot;&amp;quot;, flush=&amp;quot;&amp;quot;])&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参数&#34;&gt;参数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;*values
&lt;ul&gt;
&lt;li&gt;必需&lt;/li&gt;
&lt;li&gt;要输出的内容，可以是任何类型对象&lt;/li&gt;
&lt;li&gt;同时输出多个对象时，需要用,隔开&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;sep
&lt;ul&gt;
&lt;li&gt;可选&lt;/li&gt;
&lt;li&gt;同时输出多个对象时的分隔符&lt;/li&gt;
&lt;li&gt;默认空格&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end
&lt;ul&gt;
&lt;li&gt;可选&lt;/li&gt;
&lt;li&gt;输出最后一个对象之后的结尾符&lt;/li&gt;
&lt;li&gt;默认是一个\n，即换行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;file
&lt;ul&gt;
&lt;li&gt;可选&lt;/li&gt;
&lt;li&gt;要打印输出到的“设备”&lt;/li&gt;
&lt;li&gt;默认为输出到当前终端，可以指定已打开的文件对象&lt;/li&gt;
&lt;li&gt;对文件是否追加更新由上下文的open()中的a/w参数指定&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;flush
&lt;ul&gt;
&lt;li&gt;可选&lt;/li&gt;
&lt;li&gt;整成情况下输出是否被缓存（是否等待文件对象关闭前同一写入）由file对象决定，但如果指定为Ture则print()会强制立即将内容刷新进文件&lt;/li&gt;
&lt;li&gt;一般用不到&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Pytorch使用单机多卡训练</title>
      <link>/posts/9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/9/</guid>
      <description>需求 对基于pytorch的深度学习模型进行多卡训练以加速训练过程 由于显卡版本过于老旧，安装配置NCCL工程量过于庞大，希望使用简单的pytorch代码实现单机多卡训练，不考虑多机多卡的显卡通信 训练完成后保存的checkpoint需要能够在任何设备上进行加载、推理 实现 训练 pytorch提供了简单的单机多卡训练api，只需要在初始化模型之后执行下列语句将模型复制到多卡上 # initiate multi-gpu training model = nn.DataParallel(model, device_ids=&amp;lt;ids of the gpus you want to use&amp;gt;) 其他操作与单卡训练完全一致 加载checkpoint 上述操作后保存的checkpoint如果按照常规方法直接进行加载会报错 RuntimeError: Error(s) in loading state_dict for &amp;lt;ModelName&amp;gt;: Missing key(s) in state_dict:... debug遍历后发现其实其状态字典是完全一致的，只是因为我们在训练过程中将模型定义为了多卡并行模型。这里只需要按照训练过程中转换为多卡模型的代码初始化当前模型结构即可，即执行： # initiate multi-gpu training model = nn.DataParallel(model, device_ids=&amp;lt;ids of the gpus you want to use&amp;gt;) 其他操作与征程推理完全一致，若不想使用多卡/只想使用cpu，只需要按照常规将device = torch.device(&amp;quot;&amp;lt;cpu/cuda:id&amp;gt;&amp;quot;)即可 Note：查阅资料过程中发现有解答建议使用参数强行忽略模型加载的错误torch.load(&amp;lt;checkpoint&amp;gt;, strict=False)，经测试，这样加载的模型啥也不是&amp;hellip;不知道为什么pytorch官方要提供这个接口</description>
      <content>&lt;h2 id=&#34;需求&#34;&gt;需求&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对基于pytorch的深度学习模型进行多卡训练以加速训练过程&lt;/li&gt;
&lt;li&gt;由于显卡版本过于老旧，安装配置NCCL工程量过于庞大，希望使用简单的pytorch代码实现单机多卡训练，不考虑多机多卡的显卡通信&lt;/li&gt;
&lt;li&gt;训练完成后保存的checkpoint需要能够在任何设备上进行加载、推理&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;h3 id=&#34;训练&#34;&gt;训练&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;pytorch提供了简单的单机多卡训练api，只需要在初始化模型之后执行下列语句将模型复制到多卡上&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# initiate multi-gpu training&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataParallel(model, device_ids&lt;span style=&#34;color:#f92672&#34;&gt;=&amp;lt;&lt;/span&gt;ids of the gpus you want to use&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;其他操作与单卡训练完全一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;加载checkpoint&#34;&gt;加载checkpoint&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;上述操作后保存的checkpoint如果按照常规方法直接进行加载会报错&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;RuntimeError&lt;/span&gt;: Error(s) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; loading state_dict &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ModelName&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	Missing key(s) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; state_dict:&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;debug遍历后发现其实其状态字典是完全一致的，只是因为我们在训练过程中将模型定义为了多卡并行模型。这里只需要按照训练过程中转换为多卡模型的代码初始化当前模型结构即可，即执行：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# initiate multi-gpu training&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataParallel(model, device_ids&lt;span style=&#34;color:#f92672&#34;&gt;=&amp;lt;&lt;/span&gt;ids of the gpus you want to use&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;其他操作与征程推理完全一致，若不想使用多卡/只想使用cpu，只需要按照常规将&lt;code&gt;device = torch.device(&amp;quot;&amp;lt;cpu/cuda:id&amp;gt;&amp;quot;)&lt;/code&gt;即可&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note：查阅资料过程中发现有解答建议使用参数强行忽略模型加载的错误&lt;code&gt;torch.load(&amp;lt;checkpoint&amp;gt;, strict=False)&lt;/code&gt;，经测试，这样加载的模型啥也不是&amp;hellip;不知道为什么pytorch官方要提供这个接口&lt;/strong&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Pytorch单机多卡(DP)训练之后的模型“货不对板”</title>
      <link>/posts/20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/20/</guid>
      <description>背景 之前介绍过Pytoch的单机多卡训练，当时采用的是内置的nn.DataParallel()方法 经过单机多卡训练的模型保存checkpoint之后再次加载需要同样使用DP()对模型进行封装之后才能正常加载，否则会报错状态词典的键值对不上 原因 经过DP()封装之后的模型，其状态词典分别添加了module.的前缀，例如原参数为hr_branch.conv_hr.1.layers.0.weight，封装之后变成了module.hr_branch.conv_hr.1.layers.0.weight。 解决方案 在加载状态词典之前将该前缀进行替换，得到纯净的状态词典，则不再需要重新对模型进行DP()封装 ckp_state_dict = torch.load(args.ckp, map_location=torch.device(&amp;#34;cpu&amp;#34;))[&amp;#34;model&amp;#34;] ckp_state = {} for k, v in ckp_state_dict.items(): k = k.replace(&amp;#34;module.&amp;#34;, &amp;#34;&amp;#34;) ckp_state[k] = v 参考 文章 </description>
      <content>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;之前介绍过Pytoch的&lt;a href=&#34;https://blog.sp.chaos.autos/archives/9/&#34;&gt;单机多卡训练&lt;/a&gt;，当时采用的是内置的&lt;code&gt;nn.DataParallel()&lt;/code&gt;方法&lt;/li&gt;
&lt;li&gt;经过单机多卡训练的模型保存checkpoint之后再次加载需要同样使用&lt;code&gt;DP()&lt;/code&gt;对模型进行封装之后才能正常加载，否则会报错状态词典的键值对不上&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;原因&#34;&gt;原因&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;经过&lt;code&gt;DP()&lt;/code&gt;封装之后的模型，其状态词典分别添加了&lt;code&gt;module.&lt;/code&gt;的前缀，例如原参数为&lt;code&gt;hr_branch.conv_hr.1.layers.0.weight&lt;/code&gt;，封装之后变成了&lt;code&gt;module.hr_branch.conv_hr.1.layers.0.weight&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在加载状态词典之前将该前缀进行替换，得到纯净的状态词典，则不再需要重新对模型进行&lt;code&gt;DP()&lt;/code&gt;封装&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ckp_state_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ckp, map_location&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;device(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cpu&amp;#34;&lt;/span&gt;))[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ckp_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; ckp_state_dict&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;module.&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ckp_state[k] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://szukevin.site/2021/02/27/MODNet%E8%BD%AC%E6%88%90torchscript%E5%BD%A2%E5%BC%8F%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/&#34;&gt;文章&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>关于python协程销毁、超时</title>
      <link>/posts/2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/2/</guid>
      <description>概念 协程 通过async/await语法进行声明，是书写python异步应用的推荐方式 可等待对象 如果一个对象可以在await中使用，那么它就是可等待/awaitable对象 类型：协程、任务、Future async.sleep(delay, result=None, *, loop=None) 阻塞delay指定的秒数 sleep()总是为挂起当前任务，以允许其他任务运行 场景 Sanic后台任务执行失败被挂起，不报错，影响后续任务的进行 解决方案 对后台任务中可能出现失败的协程增设超时 asyncio.wait_for(aw,, timeout, *, loop=None) 等待aw可等待对象完成，指定timeout秒数后超时取消 timeout可以为None，也可以为float/int数值表示的等待秒数。如果timeout为None，则等待直到协程返回 如果发生超时，任务将取消并引发asyncio.TimeoutError 要避免任务取消，可以加上shield() 简单等待 asyncio.wait(aws, *, loop=None, return_when=ALL_CONPLETED) 并发地运行aws可迭代对象中的可等待对象并进入阻塞状态直到满足return_when所执行的条件 aws可迭代对象必须不为空 此函数不会引发asyncio.TimeoutError，当超时发生时，未完成的Future/Task将在指定秒数后返回；与wait_for()不同，wait()在超时后不会取消可等待对象 </description>
      <content>&lt;h2 id=&#34;概念&#34;&gt;概念&lt;/h2&gt;
&lt;h3 id=&#34;协程&#34;&gt;协程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;通过async/await语法进行声明，是书写python异步应用的推荐方式&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;可等待对象&#34;&gt;可等待对象&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;如果一个对象可以在await中使用，那么它就是可等待/awaitable对象
&lt;ul&gt;
&lt;li&gt;类型：协程、任务、Future&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;asyncsleepdelay-resultnone--loopnone&#34;&gt;async.sleep(delay, result=None, *, loop=None)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;阻塞delay指定的秒数&lt;/li&gt;
&lt;li&gt;sleep()总是为挂起当前任务，以允许其他任务运行&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;场景&#34;&gt;场景&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sanic后台任务执行失败被挂起，不报错，影响后续任务的进行&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;h3 id=&#34;对后台任务中可能出现失败的协程增设font-colorred超时font&#34;&gt;对后台任务中可能出现失败的协程增设&lt;!-- raw HTML omitted --&gt;超时&lt;!-- raw HTML omitted --&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;asyncio.wait_for(aw,, timeout, *, loop=None)
&lt;ul&gt;
&lt;li&gt;等待aw可等待对象完成，指定timeout&lt;strong&gt;秒&lt;/strong&gt;数后超时取消&lt;/li&gt;
&lt;li&gt;timeout可以为None，也可以为float/int数值表示的等待秒数。如果timeout为None，则等待直到协程返回&lt;/li&gt;
&lt;li&gt;如果发生超时，任务将取消并引发asyncio.TimeoutError&lt;/li&gt;
&lt;li&gt;要避免任务取消，可以加上shield()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;简单等待&#34;&gt;简单等待&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;asyncio.wait(aws, *, loop=None, return_when=ALL_CONPLETED)
&lt;ul&gt;
&lt;li&gt;并发地运行aws可迭代对象中的可等待对象并进入阻塞状态直到满足return_when所执行的条件&lt;/li&gt;
&lt;li&gt;aws可迭代对象必须不为空&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;此函数不会引发asyncio.TimeoutError，当超时发生时，未完成的Future/Task将在指定秒数后返回；与&lt;code&gt;wait_for()&lt;/code&gt;不同，&lt;code&gt;wait()&lt;/code&gt;在超时后不会取消可等待对象&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>深度学习中的预训练与自训练</title>
      <link>/posts/5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/5/</guid>
      <description>几个概念 预训练 自训练 自监督学习 半监督学习 监督学习 无监督学习 区分 预训练 广义上讲：是对一个模型进行“预先训练”，以完成后续的下游任务 狭义上讲（更常用）：在大规模无标注语料上，用自监督的方式训练模型 自训练 常应用于CV领域 有一个Teacher模型$M_{teacher}$和一个Student模型$M_{student}$，首先在标注数据上训练$M_{teacher}$，然后用它对大规模无标注数据进行标注，把得到的结果(与少量有标签数据混合)当做伪标注数据去训练$M_{student}$ 使用少量的标记数据和大量的未标记数据对模型进行联合训练 预训练与自训练是同级概念，其中分别可以与“监督/半监督/无监督/自监督”进行组合 监督与无监督 无监督的典型任务是聚类算法 半监督 没有太多意义的一个概念 其中的代表即自训练，甚至基本等同 自监督 是狭义上“预训练”的实现方法 与完全不受监督的设置相比，自监督学习使用数据集本身的信息来构造伪标签 是一种具有监督形式的特殊形式的非监督学习方法 </description>
      <content>&lt;h3 id=&#34;几个概念&#34;&gt;几个概念&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;预训练&lt;/li&gt;
&lt;li&gt;自训练&lt;/li&gt;
&lt;li&gt;自监督学习&lt;/li&gt;
&lt;li&gt;半监督学习&lt;/li&gt;
&lt;li&gt;监督学习&lt;/li&gt;
&lt;li&gt;无监督学习&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;区分&#34;&gt;区分&lt;/h3&gt;
&lt;h4 id=&#34;预训练&#34;&gt;预训练&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;广义上讲：是对一个模型进行“预先训练”，以完成后续的下游任务&lt;/li&gt;
&lt;li&gt;狭义上讲（更常用）：在大规模无标注语料上，用自监督的方式训练模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;自训练&#34;&gt;自训练&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;常应用于CV领域&lt;/li&gt;
&lt;li&gt;有一个Teacher模型$M_{teacher}$和一个Student模型$M_{student}$，首先在标注数据上训练$M_{teacher}$，然后用它对大规模无标注数据进行标注，把得到的结果(与少量有标签数据混合)当做伪标注数据去训练$M_{student}$&lt;/li&gt;
&lt;li&gt;使用少量的标记数据和大量的未标记数据对模型进行联合训练
&lt;strong&gt;预训练与自训练是同级概念，其中分别可以与“监督/半监督/无监督/自监督”进行组合&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;监督与无监督&#34;&gt;监督与无监督&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;无监督的典型任务是&lt;strong&gt;聚类算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;半监督&#34;&gt;半监督&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;没有太多意义的一个概念&lt;/li&gt;
&lt;li&gt;其中的代表即自训练，甚至基本等同&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;自监督&#34;&gt;自监督&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;是狭义上“预训练”的实现方法&lt;/li&gt;
&lt;li&gt;与完全不受监督的设置相比，自监督学习使用数据集本身的信息来构造伪标签&lt;/li&gt;
&lt;li&gt;是一种具有监督形式的特殊形式的非监督学习方法&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>深度神经网络中seed函数</title>
      <link>/posts/7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/7/</guid>
      <description>背景 在阅读代码过程中经常发现以下几种代码，尤其常见于pytorch书写的深度神经网络代码中 torch.manual_seed(seed) torch.cuda.manual_seed(seed) np.random.seed(seed) torch.cuda.manual_seed_all() 正文 经查询资料得出结论，该代码作用即为将模型在初始化过程中所用到的“随机数”全部固定下来，以保证每次重新训练模型需要初始化模型参数的时候能够得到相同的初始化参数，从而达到稳定复现训练结果的目的 参考资料 https://www.zhihu.com/question/288350769 https://cloud.tencent.com/developer/article/1149041 </description>
      <content>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在阅读代码过程中经常发现以下几种代码，尤其常见于&lt;code&gt;pytorch&lt;/code&gt;书写的深度神经网络代码中&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;manual_seed(seed)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;manual_seed(seed)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seed(seed)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;manual_seed_all()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;经查询资料得出结论，该代码作用即为将模型在初始化过程中所用到的“随机数”全部固定下来，以保证每次重新训练模型需要初始化模型参数的时候能够得到相同的初始化参数，从而达到稳定复现训练结果的目的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/288350769&#34;&gt;https://www.zhihu.com/question/288350769&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1149041&#34;&gt;https://cloud.tencent.com/developer/article/1149041&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
